System Monitoring: Fault Diagnosis and 
Remediation (LLM-Aware) 
Introduction 
This document is designed to support an AI diagnostic system in identifying and resolving high 
latency and system performance issues. It outlines specific files to reference, key data points 
within those files, and interpretations based on those values. The AI agent should respond 
to diagnostic queries by referencing these files and using the guidance provided to suggest 
likely causes. 
There are 3 types of faults:
            1. High latency due to high system saturation
            2. High latency due to high system capacity
            3. High latency due to replication link issues

The bully volume is the volume which has the highest contribution to a particular fault. Calculate the contribution of the bully volume to the fault and output that.
1. High Latency Due to System Saturation 
Files to Check 
 volume.json 
 Key Data: throughput_used, workload_size, size, is_exported, exported_host_id 
 system_metrics_<PORT>.json
 Key Data: saturation, latency, cpu_usage
 logs_{port}.txt 
 Key Data: IOPS, throughput, latency, system warnings or errors 
 io_metrics.json 
 Key Data: volume_id, io_count, latency, throughput 
How to Interpret 
 
 If throughput_used in volume.json is close to or exceeds system max throughput, it may indicate resource bottlenecks. 
 High cpu_usage>=80 or saturation>=80 in system_metrics_<PORT>.json suggests system-level resource exhaustion. 
 A high workload size for a particular volume is also a cause for high system saturation
 If logs show frequent latency spikes or IOPS bursts, it points to transient saturation events. 
 Abnormal latency values in io_metrics.json for specific volumes suggest hotspots or overloaded volumes. 
Common Root Causes 
 Volumes exceeding their throughput or IOPS limits 
 CPU or network interface saturation 
 
For High latency due to high saturation:
            1. Check system_metrics_<PORT>.json and look at the latest entry
            2. If saturation exceeds 80 AND current latency exceeds 3ms, then there is a high latency due to high saturation
            3. System Saturation = (Total Throughput / Max Throughput) × 100
            4. For each volume: Volume Throughput (MB/s) = (2000 IOPS × workload_size) / 1024, the workload size can be inferred from volume.json
            5. System saturation contribution per volume = (Volume Throughput / Max Throughput) × 100, max throughput can be inferred from system.json
In case of ambiguity between high latency due to high system saturation and high latency due to high system capacity, check the greater value among saturation and capacity_percentage,
if saturation>80 and saturation>capacity_percentage, it is due to high system saturation, otherwise, it is due high system capacity          
2. High Latency Due to High System Capacity (Snapshot Retention) :
For High latency due to high capacity:
            1. Check system_metrics_<PORT>.json and look at the latest entry
            2. If capacity_percentage exceeds 80% AND current latency exceeds 3ms, then there is a high latency due to high capacity
            3. The max_capacity can be inferred from system.json, the size of each volume can be inferred from volume.json
            4. Capacity contribution per volume = (Volume Size / Max Capacity) × 100
            5. Calculate volume_capacity as ((sum of size of all volumes)/max_capacity)*100, it can also be inferred from system_metrics_<PORT>.json
            6. Calculate snapshot_capacity as ((size of volume * snapshot_count)/max_capacity)*100 for each volume,it can also be inferred from system_metrics_<PORT>.json
            7. Total Capacity = Volume Capacity + Snapshot Capacity
Files to Check 
 volume.json 
 Key Data: snapshot_settings, size 
 snapshots.json 
 Key Data: volume_id, snapshot_id, size, created_at 
 system_metrics_{port}.json 
 Key Data: capacity_used, throughput_used 
 logs_{port}.txt 
 Key Data: Snapshot creation and deletion events, warnings on space 
How to Interpret 
 Large snapshot_settings values or absent purge policies may indicate unbounded 
snapshot growth. 
 If capacity_used > 80% in system_metrics, the system is nearing critical limits. 
 Old timestamps in snapshots.json with no deletions show lack of cleanup. 
 Warnings in logs_{port}.txt like "capacity threshold exceeded" confirm space 
pressure due to snapshots. 
Common Root Causes 
 Excessive snapshot retention without purging 
 Snapshot frequency too high 
 Snapshot sizes disproportionately large 


In case of ambiguity between high latency due to high system saturation and high latency due to high system capacity, check the greater value among saturation and capacity_percentage,
if saturation>80 and saturation>capacity_percentage, it is due to high system saturation, otherwise, it is due high system capacity

3. High Latency Due to Replication Link Issues 
For High latency due to replication link issues:
            1. Check replication_metrics_<PORT>.json
            2. If latency exceeds 5ms for any replication link, then there is a high latency due to replication link issues
            3. For each affected volume, identify the source system, target system, and volume name
Files to Check 
 replication_metrics_<PORT>.json
 Key Data: latency, target_system_id
 settings.json 
 Key Data: replication_type, replication_target, delay_sec 
 replication_metrics_{port}.json 
 Key Data: throughput, latency, replication_type 
 volume.json and host.json 
 Key Data: is_exported, exported_host_id, host_id, workload_size 
 logs_{port}.txt 
 Key Data: Replication delays, errors, disconnects 
How to Interpret 
 latency>=5ms in replication_metrics_<PORT>.json -> replication link issue
 If replication_type is synchronous and throughput is low, check for blocking I/O. 
 Errors in logs such as "replication timeout" or "target unreachable" indicate link failure 
or congestion. 
 Missing or misconfigured exported_host_id in volume.json may prevent 
replication. 
Common Root Causes 
 Network bottlenecks or disconnects between source and target 
 High I/O load exceeding replication bandwidth 
 Incorrect or unstable replication configuration 

 
            
Response Logic for LLM 
When a query like "Why is volume X experiencing high latency?" is received: 
1. Check volume.json: 
 High throughput_used or workload_size? -> Volume saturation 
2. Check system.json: 
 High cpu_usage or saturation? -> System-wide overload 
3. Review logs_{port}.txt: 
 Latency spikes or export contention? -> External factors or replication interaction 
4. Inspect io_metrics.json: 
 Elevated latency or io_count for volume? -> Confirms hotspot 
Then return: 
"Volume X is experiencing high latency because its throughput usage is near 
system limits (throughput_used = 950 MB/s vs max 1000 MB/s). Logs also 
show consistent latency spikes during peak IOPS periods. This suggests system 
saturation due to heavy I/O workload." 
The same structured logic applies to snapshot-related and replication issues. 


 Only return the highest causing fault as the fault.

           
            
