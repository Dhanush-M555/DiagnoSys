There are 3 types of faults:
            1. High latency due to high system saturation
            2. High latency due to high system capacity
            3. High latency due to replication link issues

1. High Latency Due to System Saturation 
Files to Check 
 volume.json 
 Key Data: throughput_used, workload_size, size, is_exported, exported_host_id 
 system_metrics_<PORT>.json
 Key Data: saturation, latency, cpu_usage
 logs_{port}.txt 
 Key Data: IOPS, throughput, latency, system warnings or errors 
 io_metrics.json 
 Key Data: volume_id, io_count, latency, throughput 
How to Interpret 
 
 If throughput_used in volume.json is close to or exceeds system max throughput, it may indicate resource bottlenecks. 
 High cpu_usage>=80 or saturation>=80 in system_metrics_<PORT>.json suggests system-level resource exhaustion. 
 A high workload size for a particular volume is also a cause for high system saturation
 If logs show frequent latency spikes or IOPS bursts, it points to transient saturation events. 
 Abnormal latency values in io_metrics.json for specific volumes suggest hotspots or overloaded volumes. 
Common Root Causes 
 Volumes exceeding their throughput or IOPS limits 
 CPU or network interface saturation 
 
For High latency due to high saturation:
            1. Check system_metrics_<PORT>.json and look at the latest entry
            2. If saturation exceeds 80 AND current latency exceeds 3ms, then there is a high latency due to high saturation
            3. System Saturation = (Total Throughput / Max Throughput) × 100
            4. For each volume: Volume Throughput (MB/s) = (2000 IOPS × workload_size) / 1024, the workload size can be inferred from volume.json
            5. System saturation contribution per volume = (Volume Throughput / Max Throughput) × 100, max throughput can be inferred from system.json
        
2. High Latency Due to High System Capacity (Snapshot Retention) :
For High latency due to high capacity:
            1. Check system_metrics_<PORT>.json and look at the latest entry
            2. If capacity_percentage exceeds 80% AND current latency exceeds 3ms, then there is a high latency due to high capacity
            3. The max_capacity can be inferred from system.json, the size of each volume can be inferred from volume.json

Files to Check 
 system_metrics_{port}.json 
 Key Data: capacity_used, throughput_used,capacity_percentage
 volume.json 
 Key Data: snapshot_settings, size , volume with the highest contribution is the bully volume
 snapshots.json 
 Key Data: volume_id, snapshot_id, size, created_at 
 logs_{port}.txt 
 Key Data: Snapshot creation and deletion events, warnings on space 
How to Interpret 
 Large snapshot_settings values or absent purge policies may indicate unbounded 
snapshot growth. 
 If capacity_used > 80% in system_metrics, the system is nearing critical limits. 
 Old timestamps in snapshots.json with no deletions show lack of cleanup. 
 Warnings in logs_{port}.txt like "capacity threshold exceeded" confirm space 
pressure due to snapshots. 
Common Root Causes 
 Excessive snapshot retention without purging 
 Snapshot frequency too high 
 Snapshot sizes disproportionately large 

3. High Latency Due to Replication Link Issues 
For High latency due to replication link issues:

            1.Check logs_<port>.txt and look for replication related lines
            If Latency is above 3ms, it means that it is a high latency due to replication link issues
            2. Check replication_metrics_<PORT>.json
            3. If latency exceeds 3ms for any replication link, then there is a high latency due to replication link issues
            4. For each affected volume, identify the source system, target system, and volume name
Files to Check 
 logs_<port>.txt
 Key Data:latency
 replication_metrics_<PORT>.json
 Key Data: latency, target_system_id
 settings.json 
 Key Data: replication_type, replication_target, delay_sec 
 replication_metrics_{port}.json 
 Key Data: throughput, latency, replication_type 
 volume.json and host.json 
 Key Data: is_exported, exported_host_id, host_id, workload_size 
 logs_{port}.txt 
 Key Data: Replication delays, errors, disconnects 
How to Interpret 
 latency>=3ms in replication_metrics_<PORT>.json and in logs_<port>.txt  -> replication link issue
 If replication_type is synchronous and throughput is low, check for blocking I/O. 
 Errors in logs such as "replication timeout" or "target unreachable" indicate link failure 
or congestion. 
 Missing or misconfigured exported_host_id in volume.json may prevent 
replication. 
Common Root Causes 
 Network bottlenecks or disconnects between source and target 
 High I/O load exceeding replication bandwidth 
 Incorrect or unstable replication configuration 

 
            
Response Logic for LLM 
When a query like "Why is volume X experiencing high latency?" is received: 
1. Check volume.json: 
 High throughput_used or workload_size? -> Volume saturation 
2. Check system.json: 
 High cpu_usage or saturation? -> System-wide overload 
3. Review logs_{port}.txt: 
 Latency spikes or export contention? -> External factors or replication interaction 
4. Inspect io_metrics.json: 
 Elevated latency or io_count for volume? -> Confirms hotspot 
Then return: 
"Volume X is experiencing high latency because its throughput usage is near 
system limits (throughput_used = 950 MB/s vs max 1000 MB/s). Logs also 
show consistent latency spikes during peak IOPS periods. This suggests system 
saturation due to heavy I/O workload." 
The same structured logic applies to snapshot-related and replication issues. 


 Only return the highest causing fault as the fault.

           
            
